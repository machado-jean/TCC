{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48367734",
   "metadata": {},
   "source": [
    "# Aquisição de dados no portal da ONS Dados Aberto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4d4a1",
   "metadata": {},
   "source": [
    "https://ons-aws-prod-opendata.s3.amazonaws.com/\n",
    "\n",
    "https://registry.opendata.aws/ons-opendata-portal/\n",
    "\n",
    "https://github.com/awslabs/open-data-registry/blob/main/datasets/ons-opendata-portal.yaml#L17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dab1c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime,  timedelta\n",
    "import sys\n",
    "# Adiciona o caminho da pasta src ao sys.path para importar datasets_ons\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "from datasets_ons import datasets_ons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b9e1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_arquivos():\n",
    "    url = \"https://ons-aws-prod-opendata.s3.amazonaws.com\"\n",
    "    params = {\"list-type\": \"2\", \"prefix\": \"dataset/\"}\n",
    "    arquivos = []\n",
    "    while True:\n",
    "        resp = requests.get(url, params=params)\n",
    "        root = ET.fromstring(resp.content)\n",
    "        for item in root.findall(\".//{*}Contents\"):\n",
    "            key_elem = item.find(\"{*}Key\")\n",
    "            if key_elem is not None and key_elem.text is not None:\n",
    "                arquivos.append(key_elem.text)\n",
    "        token = root.find(\".//{*}NextContinuationToken\")\n",
    "        if token is not None and token.text is not None:\n",
    "            params[\"continuation-token\"] = token.text\n",
    "        else:\n",
    "            break\n",
    "    return arquivos\n",
    "\n",
    "def listar_arquivos_do_dataset(nome_dataset=None, extensoes=(\".csv\", \".parquet\", \".xlsx\", \".pdf\")):\n",
    "    todos = listar_arquivos()\n",
    "    if nome_dataset is None:\n",
    "        datasets = sorted({arq.split(\"/\")[1] for arq in todos if arq.startswith(\"dataset/\") and not arq.endswith(\"/\")})\n",
    "        print(f\"\\nDatasets disponíveis ({datetime.now().strftime('%d/%m/%y')}):\")\n",
    "        for ds in datasets:\n",
    "            print(\"-\", ds)\n",
    "        return\n",
    "\n",
    "    arquivos = [arq.split(\"/\")[-1] for arq in todos\n",
    "                if arq.startswith(f\"dataset/{nome_dataset}/\") and not arq.endswith(\"/\")\n",
    "                and any(arq.endswith(ext) for ext in extensoes)]\n",
    "\n",
    "    if not arquivos:\n",
    "        print(f\"Nenhum arquivo encontrado para o dataset '{nome_dataset}'.\")\n",
    "    else:\n",
    "        print(f\"\\nArquivos do dataset '{nome_dataset}':\")\n",
    "        for arq in arquivos:\n",
    "            print(\"-\", arq)\n",
    "\n",
    "def baixar_arquivo(nome_dataset, nome_arquivo, pasta_destino):\n",
    "    url = f\"https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/{nome_dataset}/{nome_arquivo}\"\n",
    "    os.makedirs(pasta_destino, exist_ok=True)\n",
    "    caminho = os.path.join(pasta_destino, nome_arquivo)\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(caminho, 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        print(f\"Baixado: {nome_arquivo}\")\n",
    "    else:\n",
    "        print(f\"Erro {resp.status_code} ao baixar: {nome_arquivo}\")\n",
    "\n",
    "def baixar_dataset_ons(nome_dataset, pasta_destino, data_inicio=None, data_fim=None):\n",
    "    \"\"\"\n",
    "    Baixa arquivos públicos do ONS a partir do dataset informado, com base na frequência dos dados\n",
    "    (anual, mensal, diário ou fixo) definida no dicionário datasets_ons.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    nome_dataset : str\n",
    "        Nome do dataset disponível no bucket ONS.\n",
    "    pasta_destino : str\n",
    "        Caminho onde os arquivos serão salvos localmente.\n",
    "    data_inicio : str, opcional\n",
    "        Data inicial no formato \"YYYY\", \"YYYY-MM\" ou \"YYYY-MM-DD\", dependendo da frequência do dataset.\n",
    "        Obrigatória para datasets não fixos.\n",
    "    data_fim : str, opcional\n",
    "        Data final no formato \"YYYY\", \"YYYY-MM\" ou \"YYYY-MM-DD\", dependendo da frequência do dataset.\n",
    "        Obrigatória para datasets não fixos.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    None\n",
    "        A função não retorna nada. Os arquivos são salvos localmente na pasta especificada.\n",
    "    \"\"\"\n",
    "    if nome_dataset not in datasets_ons:\n",
    "        print(f\"Dataset '{nome_dataset}' não encontrado.\")\n",
    "        return\n",
    "\n",
    "    info = datasets_ons[nome_dataset]\n",
    "    padrao, freq = info['padrao'], info['frequencia']\n",
    "\n",
    "    if freq == 'fixo':\n",
    "        baixar_arquivo(nome_dataset, padrao, pasta_destino)\n",
    "        return\n",
    "\n",
    "    if not data_inicio or not data_fim:\n",
    "        print(f\"Datas de início e fim são obrigatórias para datasets do tipo '{freq}'.\")\n",
    "        return\n",
    "\n",
    "    def parse_data(data):\n",
    "        try:\n",
    "            return datetime.strptime(data, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return datetime.strptime(data, \"%Y-%m\")\n",
    "            except ValueError:\n",
    "                return datetime.strptime(data, \"%Y\")\n",
    "\n",
    "    dt_ini = parse_data(data_inicio)\n",
    "    dt_fim = parse_data(data_fim)\n",
    "    nomes = []\n",
    "\n",
    "    if freq == 'anual':\n",
    "        for ano in range(dt_ini.year, dt_fim.year + 1):\n",
    "            nomes.append(padrao.format(ano=ano))\n",
    "\n",
    "    elif freq == 'mensal':\n",
    "        atual = datetime(dt_ini.year, dt_ini.month, 1)\n",
    "        while atual <= dt_fim:\n",
    "            nomes.append(padrao.format(ano=atual.year, mes=atual.month))\n",
    "            ano = atual.year + (atual.month // 12)\n",
    "            mes = atual.month % 12 + 1\n",
    "            atual = datetime(ano, mes, 1)\n",
    "\n",
    "    elif freq == 'diario':\n",
    "        atual = dt_ini\n",
    "        while atual <= dt_fim:\n",
    "            nomes.append(padrao.format(ano=atual.year, mes=atual.month, dia=atual.day))\n",
    "            atual += timedelta(days=1)\n",
    "\n",
    "    for nome_arq in nomes:\n",
    "        baixar_arquivo(nome_dataset, nome_arq, pasta_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c27e3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixado: CURVA_CARGA_2000.csv\n",
      "Baixado: CURVA_CARGA_2001.csv\n",
      "Baixado: CURVA_CARGA_2002.csv\n",
      "Baixado: CURVA_CARGA_2003.csv\n",
      "Baixado: CURVA_CARGA_2004.csv\n",
      "Baixado: CURVA_CARGA_2005.csv\n",
      "Baixado: CURVA_CARGA_2006.csv\n",
      "Baixado: CURVA_CARGA_2007.csv\n",
      "Baixado: CURVA_CARGA_2008.csv\n",
      "Baixado: CURVA_CARGA_2009.csv\n",
      "Baixado: CURVA_CARGA_2010.csv\n",
      "Baixado: CURVA_CARGA_2011.csv\n",
      "Baixado: CURVA_CARGA_2012.csv\n",
      "Baixado: CURVA_CARGA_2013.csv\n",
      "Baixado: CURVA_CARGA_2014.csv\n",
      "Baixado: CURVA_CARGA_2015.csv\n",
      "Baixado: CURVA_CARGA_2016.csv\n",
      "Baixado: CURVA_CARGA_2017.csv\n",
      "Baixado: CURVA_CARGA_2018.csv\n",
      "Baixado: CURVA_CARGA_2019.csv\n",
      "Baixado: CURVA_CARGA_2020.csv\n",
      "Baixado: CURVA_CARGA_2021.csv\n",
      "Baixado: CURVA_CARGA_2022.csv\n",
      "Baixado: CURVA_CARGA_2023.csv\n",
      "Baixado: CURVA_CARGA_2024.csv\n",
      "Baixado: CURVA_CARGA_2025.csv\n"
     ]
    }
   ],
   "source": [
    "baixar_dataset_ons(\"curva-carga-ho\", pasta_destino=\"../data/Curva Carga HO ONS\", data_inicio=\"2000-01-01\", data_fim=\"2025-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8fc02f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
