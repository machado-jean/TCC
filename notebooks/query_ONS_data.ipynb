{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48367734",
   "metadata": {},
   "source": [
    "# Aquisição de dados no portal da ONS Dados Aberto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4d4a1",
   "metadata": {},
   "source": [
    "https://ons-aws-prod-opendata.s3.amazonaws.com/\n",
    "\n",
    "https://registry.opendata.aws/ons-opendata-portal/\n",
    "\n",
    "https://github.com/awslabs/open-data-registry/blob/main/datasets/ons-opendata-portal.yaml#L17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dab1c70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime,  timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15f3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona o caminho da pasta src ao sys.path para importar datasets_ons\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\", \"src\")))\n",
    "from datasets_ons import datasets_ons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9e1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listar_arquivos():\n",
    "    url = \"https://ons-aws-prod-opendata.s3.amazonaws.com\"\n",
    "    params = {\"list-type\": \"2\", \"prefix\": \"dataset/\"}\n",
    "    arquivos = []\n",
    "    while True:\n",
    "        resp = requests.get(url, params=params)\n",
    "        root = ET.fromstring(resp.content)\n",
    "        for item in root.findall(\".//{*}Contents\"):\n",
    "            key_elem = item.find(\"{*}Key\")\n",
    "            if key_elem is not None and key_elem.text is not None:\n",
    "                arquivos.append(key_elem.text)\n",
    "        token = root.find(\".//{*}NextContinuationToken\")\n",
    "        if token is not None and token.text is not None:\n",
    "            params[\"continuation-token\"] = token.text\n",
    "        else:\n",
    "            break\n",
    "    return arquivos\n",
    "\n",
    "\n",
    "def listar_arquivos_do_dataset(nome_dataset=None, extensoes=(\".csv\", \".parquet\", \".xlsx\", \".pdf\")):\n",
    "    todos = listar_arquivos()\n",
    "    if nome_dataset is None:\n",
    "        datasets = sorted({arq.split(\n",
    "            \"/\")[1] for arq in todos if arq.startswith(\"dataset/\") and not arq.endswith(\"/\")})\n",
    "        print(\n",
    "            f\"\\nDatasets disponíveis ({datetime.now().strftime('%d/%m/%y')}):\")\n",
    "        for ds in datasets:\n",
    "            print(\"-\", ds)\n",
    "        return\n",
    "\n",
    "    arquivos = [arq.split(\"/\")[-1] for arq in todos\n",
    "                if arq.startswith(f\"dataset/{nome_dataset}/\") and not arq.endswith(\"/\")\n",
    "                and any(arq.endswith(ext) for ext in extensoes)]\n",
    "\n",
    "    if not arquivos:\n",
    "        print(f\"Nenhum arquivo encontrado para o dataset '{nome_dataset}'.\")\n",
    "    else:\n",
    "        print(f\"\\nArquivos do dataset '{nome_dataset}':\")\n",
    "        for arq in arquivos:\n",
    "            print(\"-\", arq)\n",
    "\n",
    "\n",
    "def baixar_arquivo(nome_dataset, nome_arquivo, pasta_destino):\n",
    "    url = f\"https://ons-aws-prod-opendata.s3.amazonaws.com/dataset/{nome_dataset}/{nome_arquivo}\"\n",
    "    os.makedirs(pasta_destino, exist_ok=True)\n",
    "    caminho = os.path.join(pasta_destino, nome_arquivo)\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code == 200:\n",
    "        with open(caminho, 'wb') as f:\n",
    "            f.write(resp.content)\n",
    "        print(f\"Baixado: {nome_arquivo}\")\n",
    "    else:\n",
    "        print(f\"Erro {resp.status_code} ao baixar: {nome_arquivo}\")\n",
    "\n",
    "\n",
    "def baixar_dataset_ons(nome_dataset, pasta_destino, data_inicio=None, data_fim=None):\n",
    "    \"\"\"\n",
    "    Baixa arquivos públicos do ONS a partir do dataset informado, com base na frequência dos dados\n",
    "    (anual, mensal, diário ou fixo) definida no dicionário datasets_ons.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    nome_dataset : str\n",
    "        Nome do dataset disponível no bucket ONS.\n",
    "    pasta_destino : str\n",
    "        Caminho onde os arquivos serão salvos localmente.\n",
    "    data_inicio : str, opcional\n",
    "        Data inicial no formato \"YYYY\", \"YYYY-MM\" ou \"YYYY-MM-DD\", dependendo da frequência do dataset.\n",
    "        Obrigatória para datasets não fixos.\n",
    "    data_fim : str, opcional\n",
    "        Data final no formato \"YYYY\", \"YYYY-MM\" ou \"YYYY-MM-DD\", dependendo da frequência do dataset.\n",
    "        Obrigatória para datasets não fixos.\n",
    "\n",
    "    Retorna:\n",
    "    -------\n",
    "    None\n",
    "        A função não retorna nada. Os arquivos são salvos localmente na pasta especificada.\n",
    "    \"\"\"\n",
    "    if nome_dataset not in datasets_ons:\n",
    "        print(f\"Dataset '{nome_dataset}' não encontrado.\")\n",
    "        return\n",
    "\n",
    "    info = datasets_ons[nome_dataset]\n",
    "    padrao, freq = info['padrao'], info['frequencia']\n",
    "\n",
    "    if freq == 'fixo':\n",
    "        baixar_arquivo(nome_dataset, padrao, pasta_destino)\n",
    "        return\n",
    "\n",
    "    if not data_inicio or not data_fim:\n",
    "        print(\n",
    "            f\"Datas de início e fim são obrigatórias para datasets do tipo '{freq}'.\")\n",
    "        return\n",
    "\n",
    "    def parse_data(data):\n",
    "        try:\n",
    "            return datetime.strptime(data, \"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return datetime.strptime(data, \"%Y-%m\")\n",
    "            except ValueError:\n",
    "                return datetime.strptime(data, \"%Y\")\n",
    "\n",
    "    dt_ini = parse_data(data_inicio)\n",
    "    dt_fim = parse_data(data_fim)\n",
    "    nomes = []\n",
    "\n",
    "    if freq == 'anual':\n",
    "        for ano in range(dt_ini.year, dt_fim.year + 1):\n",
    "            nomes.append(padrao.format(ano=ano))\n",
    "\n",
    "    elif freq == 'mensal':\n",
    "        atual = datetime(dt_ini.year, dt_ini.month, 1)\n",
    "        while atual <= dt_fim:\n",
    "            nomes.append(padrao.format(ano=atual.year, mes=atual.month))\n",
    "            ano = atual.year + (atual.month // 12)\n",
    "            mes = atual.month % 12 + 1\n",
    "            atual = datetime(ano, mes, 1)\n",
    "\n",
    "    elif freq == 'diario':\n",
    "        atual = dt_ini\n",
    "        while atual <= dt_fim:\n",
    "            nomes.append(padrao.format(ano=atual.year,\n",
    "                         mes=atual.month, dia=atual.day))\n",
    "            atual += timedelta(days=1)\n",
    "\n",
    "    for nome_arq in nomes:\n",
    "        baixar_arquivo(nome_dataset, nome_arq, pasta_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c27e3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixado: CURVA_CARGA_2025.csv\n"
     ]
    }
   ],
   "source": [
    "baixar_dataset_ons(\"curva-carga-ho\", pasta_destino=\"../data\", data_inicio=\"2025-01-01\", data_fim=\"2025-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8fc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivotar_curva_carga_subsistemas(caminho_arquivo):\n",
    "    \"\"\"\n",
    "    Lê um arquivo CSV da curva de carga por subsistema (no formato longo) e transforma para o formato largo,\n",
    "    onde cada subsistema (Norte, Nordeste, Sul, Sudeste) se torna uma coluna e cada linha representa um timestamp.\n",
    "\n",
    "    Parâmetros:\n",
    "    ----------\n",
    "    caminho_arquivo : str\n",
    "        Caminho para o arquivo CSV de entrada, separado por ponto e vírgula.\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    df_largo : pandas.DataFrame\n",
    "        DataFrame com uma linha por timestamp e colunas para cada subsistema.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(caminho_arquivo, sep=\";\")\n",
    "\n",
    "    df_largo = df.pivot(\n",
    "        index=\"din_instante\", columns=\"nom_subsistema\", values=\"val_cargaenergiahomwmed\")\n",
    "\n",
    "    df_largo = df_largo.rename(columns={\n",
    "        \"NORTE\": \"Norte\",\n",
    "        \"NORDESTE\": \"Nordeste\",\n",
    "        \"SUL\": \"Sul\",\n",
    "        \"SUDESTE\": \"Sudeste\"\n",
    "    })\n",
    "\n",
    "    colunas_desejadas = [\"Norte\", \"Nordeste\", \"Sul\", \"Sudeste\"]\n",
    "    colunas_presentes = [\n",
    "        col for col in colunas_desejadas if col in df_largo.columns]\n",
    "    df_largo = df_largo[colunas_presentes]\n",
    "\n",
    "    df_largo = df_largo.reset_index()\n",
    "\n",
    "    return df_largo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b72a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preencher_horas_faltantes(df, metodo='spline', order=2):\n",
    "    \"\"\"\n",
    "    Preenche as horas faltantes no índice datetime do DataFrame, interpolando os valores de acordo com o método escolhido.\n",
    "\n",
    "    Parâmetros:\n",
    "    df (pd.DataFrame): DataFrame com índice datetime e colunas numéricas.\n",
    "    metodo (str): Método de interpolação ('linear', 'time', 'index', 'values', 'pad', \n",
    "                  'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'barycentric', \n",
    "                  'polynomial', 'krogh', 'piecewise_polynomial', 'spline', 'pchip', \n",
    "                  'akima', 'cubicspline', 'from_derivatives').\n",
    "                  Padrão: 'spline'.\n",
    "    order (int, opcional): Ordem do polinômio para os métodos 'polynomial' e 'spline'. Padrão: 2.\n",
    "\n",
    "    Retorna:\n",
    "    pd.DataFrame: DataFrame com as horas corrigidas e valores interpolados.\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"O DataFrame está vazio. Nenhuma interpolação foi realizada.\")\n",
    "        return df\n",
    "\n",
    "    # Garante que o índice é datetime e remove valores inválidos\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "\n",
    "    if df.index.isna().any():\n",
    "        raise ValueError(\n",
    "            \"O índice contém valores não reconhecidos como datas.\")\n",
    "\n",
    "    # Criar um range de tempo contínuo com frequência de 1 hora\n",
    "    full_range = pd.date_range(\n",
    "        start=df.index.min(), end=df.index.max(), freq='h')\n",
    "\n",
    "    # Só reindexa se houver horas ausentes\n",
    "    if not df.index.equals(full_range):\n",
    "        df = df.reindex(full_range)\n",
    "\n",
    "    # Filtra apenas colunas numéricas\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    non_numeric_cols = df.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "    if numeric_cols.empty:\n",
    "        print(\"Nenhuma coluna numérica encontrada para interpolação. Retornando DataFrame original.\")\n",
    "        return df\n",
    "\n",
    "    # Lista de métodos válidos\n",
    "    metodos_validos = [\n",
    "        'linear', 'time', 'index', 'values', 'pad', 'nearest', 'zero',\n",
    "        'slinear', 'quadratic', 'cubic', 'barycentric', 'polynomial', 'krogh',\n",
    "        'piecewise_polynomial', 'spline', 'pchip', 'akima', 'cubicspline', 'from_derivatives'\n",
    "    ]\n",
    "\n",
    "    # Aplicando o método de interpolação escolhido\n",
    "    try:\n",
    "        if metodo in metodos_validos:\n",
    "            if metodo in ['spline', 'polynomial'] and order is None:\n",
    "                order = 2  # Define um padrão se o usuário não passar\n",
    "            df[numeric_cols] = df[numeric_cols].interpolate(\n",
    "                method=metodo, order=order if metodo in ['spline', 'polynomial'] else None)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Método de interpolação inválido: '{metodo}'. Métodos disponíveis: {', '.join(metodos_validos)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao interpolar os dados: {e}\")\n",
    "        return df\n",
    "\n",
    "    # Aviso se existirem colunas não numéricas\n",
    "    if not non_numeric_cols.empty:\n",
    "        print(\n",
    "            f\"As colunas não numéricas foram ignoradas: {list(non_numeric_cols)}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bde20461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_pasta_curva_carga(pasta_entrada, salvar_em=None, metodo_interp='spline', ordem_interp=2):\n",
    "    \"\"\"\n",
    "    Processa todos os arquivos CSV de curva de carga em uma pasta, transforma os dados para formato largo\n",
    "    (pivotando os subsistemas) e concatena todos em um único DataFrame.\n",
    "\n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    pasta_entrada : str\n",
    "        Caminho para a pasta contendo os arquivos CSV.\n",
    "    salvar_em : str, opcional\n",
    "        Caminho para salvar o arquivo consolidado. Se None, não salva.\n",
    "\n",
    "    Retorno:\n",
    "    --------\n",
    "    df_final : pandas.DataFrame\n",
    "        DataFrame unificado com os dados transformados de todos os arquivos.\n",
    "    \"\"\"\n",
    "    arquivos_csv = sorted([\n",
    "        os.path.join(pasta_entrada, f)\n",
    "        for f in os.listdir(pasta_entrada)\n",
    "        if f.endswith(\".csv\")\n",
    "    ])\n",
    "\n",
    "    dfs = []\n",
    "    for caminho in arquivos_csv:\n",
    "        try:\n",
    "            df = pivotar_curva_carga_subsistemas(caminho)\n",
    "            df['din_instante'] = pd.to_datetime(df['din_instante'])\n",
    "            df.set_index('din_instante', inplace=True)\n",
    "            print(f\"Processando arquivo: {caminho}\")\n",
    "            df = preencher_horas_faltantes(\n",
    "                df, metodo=metodo_interp, order=ordem_interp)\n",
    "            print(f\"Arquivo processado: {caminho} - {len(df)} linhas\")\n",
    "            if df.empty:\n",
    "                print(f\"Arquivo {caminho} está vazio após o processamento.\")\n",
    "                continue\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar {caminho}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(\"Nenhum arquivo válido encontrado na pasta.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_final = pd.concat(dfs).sort_index().reset_index()\n",
    "\n",
    "    if salvar_em:\n",
    "        os.makedirs(os.path.dirname(salvar_em), exist_ok=True)\n",
    "        df_final.to_csv(salvar_em, index=False)\n",
    "        print(f\"Arquivo consolidado salvo em: {salvar_em}\")\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cecd8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2000.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2000.csv - 8784 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2001.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2001.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2002.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2002.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2003.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2003.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2004.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2004.csv - 8784 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2005.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2005.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2006.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2006.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2007.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2007.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2008.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2008.csv - 8784 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2009.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2009.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2010.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2010.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2011.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2011.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2012.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2012.csv - 8784 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2013.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2013.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2014.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2014.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2015.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2015.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2016.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2016.csv - 8784 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2017.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2017.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2018.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2018.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2019.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2019.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2020.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2020.csv - 8784 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2021.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2021.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2022.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2022.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2023.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2023.csv - 8760 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2024.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2024.csv - 8784 linhas\n",
      "Processando arquivo: ../data/Curva Carga HO ONS\\CURVA_CARGA_2025.csv\n",
      "Arquivo processado: ../data/Curva Carga HO ONS\\CURVA_CARGA_2025.csv - 4032 linhas\n",
      "Arquivo consolidado salvo em: ../data/curva_carga_ho_2000_2025.csv\n"
     ]
    }
   ],
   "source": [
    "curva2000 = processar_pasta_curva_carga(\n",
    "    \"../data/Curva Carga HO ONS\",\n",
    "    \"../data/curva_carga_ho_2000_2025.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
